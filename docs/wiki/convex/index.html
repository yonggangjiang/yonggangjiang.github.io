<!DOCTYPE html><meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<style>
  .blackout {
    background-color: black;
    color: black;
    cursor: pointer;
    transition: color 0.3s ease;
    padding: 2px 4px;
    border-radius: 4px;
  }

  .blackout.revealed {
    color: white;  
  }
</style>


<title>Convex Optimization | Home | Yonggang&#39;s homepage</title>

<meta name="generator" content="Hugo Eureka 0.8.3-dev" />
<link rel="stylesheet" href="https://jyg94.github.io/css/eureka.min.css">
<script defer src="https://jyg94.github.io/js/eureka.min.js"></script>

<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&family=Noto+Serif+SC:wght@400;600;700&display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/styles/solarized-light.min.css"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/highlight.min.js"
   crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/dart.min.js"
     crossorigin></script>

<script defer src="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/js/all.min.js"
   integrity="sha256-uNYoXefWRqv&#43;PsIF/OflNmwtKM4lStn9yrz2gVl6ymo="  crossorigin></script>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
   integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3&#43;Aro6EYUG4&#43;cU&#43;KJWu/X"  media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" 
  integrity="sha384-g7c&#43;Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI&#43;sEnkvrMWph2EDg4"  crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
   integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC&#43;Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ],
    });
  });
</script>


<script defer src="https://cdn.jsdelivr.net/npm/mermaid@8.9.2/dist/mermaid.min.js" 
  integrity="sha256-Zmpaaj&#43;GXFsPF5WdPArSrnW3b30dovldeKsW00xBVwE="  crossorigin></script>


<link rel="icon" type="image/png" sizes="32x32" href="https://jyg94.github.io/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_32x32_fill_box_center_3.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://jyg94.github.io/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_180x180_fill_box_center_3.png">

<meta name="description"
  content="Convex Optimization A set $K\subseteq\mathbb{R}^n$ is called convex if for any $x,y\in K,\lambda\in[0,1]$ we have $\lambda x&#43;(1-\lambda)y\in K$.
A function $f:K\to\mathbb{R}^n$ is called a convex function if $K$ is convex and for any $x,y\in K,\lambda\in[0,1]$ we have $\lambda f(x)&#43;(1-\lambda)f(y)\ge f(\lambda x&#43;(1-\lambda)y)$. In other words, the set $\lbrace(x,y)|y\ge f(x)\rbrace$ is a convex set in $\mathbb{R}^{n&#43;1}$.
Supporting Hyperplane and Subgradients Seperating and Supporting Hyperplane Theorem Suppose $K\subseteq\mathbb{R}^n$ is convex and closed. Then for any $y^*\in\mathbb{R}^n$ which is not a interior node of $K$, there exists $h\in\mathbb{R}^n\backslash\lbrace \mathbf{0}\rbrace$ such that for any $x\in K$ we have  \[\langle x,h\rangle\le\langle y^*,h\rangle\]  i.">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"Docs",
      "item":"https://jyg94.github.io/docs/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"Home",
      "item":"https://jyg94.github.io/docs/wiki/"},{
      "@type": "ListItem",
      "position": 3 ,
      "name":"Convex Optimization",
      "item":"https://jyg94.github.io/docs/wiki/convex/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://jyg94.github.io/docs/wiki/convex/"
    },
    "headline": "Convex Optimization | Home | Yonggang\u0027s homepage","datePublished": "2020-11-20T22:52:56+08:00",
    "dateModified": "2020-11-20T22:52:56+08:00",
    "wordCount":  4136 ,
    "publisher": {
        "@type": "Person",
        "name": "C. Wang",
        "logo": {
            "@type": "ImageObject",
            "url": "https://jyg94.github.io/images/icon.png"
        }
        },
    "description": "Convex Optimization A set $K\\subseteq\\mathbb{R}^n$ is called convex if for any $x,y\\in K,\\lambda\\in[0,1]$ we have $\\lambda x\u002b(1-\\lambda)y\\in K$.\nA function $f:K\\to\\mathbb{R}^n$ is called a convex function if $K$ is convex and for any $x,y\\in K,\\lambda\\in[0,1]$ we have $\\lambda f(x)\u002b(1-\\lambda)f(y)\\ge f(\\lambda x\u002b(1-\\lambda)y)$. In other words, the set $\\lbrace(x,y)|y\\ge f(x)\\rbrace$ is a convex set in $\\mathbb{R}^{n\u002b1}$.\nSupporting Hyperplane and Subgradients Seperating and Supporting Hyperplane Theorem\r\r Suppose $K\\subseteq\\mathbb{R}^n$ is convex and closed. Then for any $y^*\\in\\mathbb{R}^n$ which is not a interior node of $K$, there exists $h\\in\\mathbb{R}^n\\backslash\\lbrace \\mathbf{0}\\rbrace$ such that for any $x\\in K$ we have  \\[\\langle x,h\\rangle\\le\\langle y^*,h\\rangle\\]  i."
}
</script><meta property="og:title" content="Convex Optimization | Home | Yonggang&#39;s homepage" />
<meta property="og:type" content="article" />


<meta property="og:image" content="https://jyg94.github.io/images/icon.png">


<meta property="og:url" content="https://jyg94.github.io/docs/wiki/convex/" />




<meta property="og:description" content="Convex Optimization A set $K\subseteq\mathbb{R}^n$ is called convex if for any $x,y\in K,\lambda\in[0,1]$ we have $\lambda x&#43;(1-\lambda)y\in K$.
A function $f:K\to\mathbb{R}^n$ is called a convex function if $K$ is convex and for any $x,y\in K,\lambda\in[0,1]$ we have $\lambda f(x)&#43;(1-\lambda)f(y)\ge f(\lambda x&#43;(1-\lambda)y)$. In other words, the set $\lbrace(x,y)|y\ge f(x)\rbrace$ is a convex set in $\mathbb{R}^{n&#43;1}$.
Supporting Hyperplane and Subgradients Seperating and Supporting Hyperplane Theorem Suppose $K\subseteq\mathbb{R}^n$ is convex and closed. Then for any $y^*\in\mathbb{R}^n$ which is not a interior node of $K$, there exists $h\in\mathbb{R}^n\backslash\lbrace \mathbf{0}\rbrace$ such that for any $x\in K$ we have  \[\langle x,h\rangle\le\langle y^*,h\rangle\]  i." />




<meta property="og:locale" content="en" />




<meta property="og:site_name" content="Yonggang&#39;s homepage" />






<meta property="article:published_time" content="2020-11-20T22:52:56&#43;08:00" />


<meta property="article:modified_time" content="2020-11-20T22:52:56&#43;08:00" />



<meta property="article:section" content="docs" />




<head>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-94BJ1L4MWP"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-94BJ1L4MWP');
</script>
</head>

<body class="flex flex-col min-h-screen">
  <header class="fixed flex items-center w-full min-h-16 z-50 bg-secondary-bg shadow-sm">
    <div class="w-full mx-auto"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0" style="width:1300px">
    <a href="/docs/wiki" class="mr-6 text-primary-text text-xl font-bold" style="color: black;">Yonggang's wiki</a>
    

    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change', switchDarkMode)
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }

    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script>
</div>
  </header>
  <main class="flex-grow pt-16">


<div class="flex flex-col md:flex-row bg-secondary-bg rounded">
    <div class="md:w-1/4 lg:w-1/6 border-r">
        <div class="sticky top-16 pt-6">
            










<div id="sidebar-title" class="md:hidden mx-4 px-2 pt-4 pb-2 md:border-b text-tertiary-text md:text-primary-text">
    <span class="font-semibold">Table of Contents</span>
    <i class="fas fa-caret-right ml-1"></i>
</div>

<div id="sidebar-toc"
    class="hidden md:block overflow-y-auto mx-6 md:mx-0 pr-6 pt-2 md:max-h-doc-sidebar bg-primary-bg md:bg-transparent">

    <div class="flex flex-wrap ml-4 -mr-2 p-2 hover:bg-primary-bg rounded" style="cursor:pointer" onclick="window.location.href=&#34;https://jyg94.github.io/docs/wiki/&#34;">
        <a class=" hover:text-eureka font-bold"
            href="https://jyg94.github.io/docs/wiki/">Home</a>
        
        
        


    </div>
    
    


<ul class="pl-4">
    
    
    
    <li class="py-1"></li>
        <div class="  -mr-2 p-2  hover:bg-primary-bg rounded" style="cursor:pointer" onclick="window.location.href=&#34;https://jyg94.github.io/docs/wiki/advanced/&#34;">
            <a class=""
            href="https://jyg94.github.io/docs/wiki/advanced/">Advanced Algorithm (ETHz)</a>
        </div>
        
    </li>
    
    
    
    
    <li class="py-1"></li>
        <div class="  -mr-2 p-2  hover:bg-primary-bg rounded" style="cursor:pointer" onclick="window.location.href=&#34;https://jyg94.github.io/docs/wiki/math/&#34;">
            <a class=""
            href="https://jyg94.github.io/docs/wiki/math/">Basic Math</a>
        </div>
        
    </li>
    
    
    
    
    <li class="py-1"></li>
        <div class="  -mr-2 p-2  hover:bg-primary-bg rounded" style="cursor:pointer" onclick="window.location.href=&#34;https://jyg94.github.io/docs/wiki/complexity/&#34;">
            <a class=""
            href="https://jyg94.github.io/docs/wiki/complexity/">Complexity</a>
        </div>
        
    </li>
    
    
    
    
    <li class="py-1"></li>
        <div class="  -mr-2 p-2  hover:bg-primary-bg rounded" style="cursor:pointer" onclick="window.location.href=&#34;https://jyg94.github.io/docs/wiki/concentration/&#34;">
            <a class=""
            href="https://jyg94.github.io/docs/wiki/concentration/">Concentration of Measure</a>
        </div>
        
    </li>
    
    
    
    
    <li class="py-1"></li>
        <div class="  -mr-2 p-2  hover:bg-primary-bg rounded" style="cursor:pointer" onclick="window.location.href=&#34;https://jyg94.github.io/docs/wiki/convex/&#34;">
            <a class=" text-eureka "
            href="https://jyg94.github.io/docs/wiki/convex/">Convex Optimization</a>
        </div>
        
    </li>
    
    
    
    
    <li class="py-1"></li>
        <div class="  -mr-2 p-2  hover:bg-primary-bg rounded" style="cursor:pointer" onclick="window.location.href=&#34;https://jyg94.github.io/docs/wiki/finegrained/&#34;">
            <a class=""
            href="https://jyg94.github.io/docs/wiki/finegrained/">Fine-Grained Complexity</a>
        </div>
        
    </li>
    
    
    
    
    <li class="py-1"></li>
        <div class="  -mr-2 p-2  hover:bg-primary-bg rounded" style="cursor:pointer" onclick="window.location.href=&#34;https://jyg94.github.io/docs/wiki/graph/&#34;">
            <a class=""
            href="https://jyg94.github.io/docs/wiki/graph/">Graph Theory</a>
        </div>
        
    </li>
    
    
    
    
    <li class="py-1"></li>
        <div class="  -mr-2 p-2  hover:bg-primary-bg rounded" style="cursor:pointer" onclick="window.location.href=&#34;https://jyg94.github.io/docs/wiki/matrix/&#34;">
            <a class=""
            href="https://jyg94.github.io/docs/wiki/matrix/">Matrix</a>
        </div>
        
    </li>
    
    
    
    
    <li class="py-1"></li>
        <div class="  -mr-2 p-2  hover:bg-primary-bg rounded" style="cursor:pointer" onclick="window.location.href=&#34;https://jyg94.github.io/docs/wiki/ml/&#34;">
            <a class=""
            href="https://jyg94.github.io/docs/wiki/ml/">ML Foundations</a>
        </div>
        
    </li>
    
    
    <li class="py-2"></li>
    </li>
</ul>

</div>





        </div>
    </div>
    <div class="w-full md:w-3/4 lg:w-4/5 pb-8 pt-2 md:pt-8">
        <div class="flex">
            <div class="w-full lg:w-4/4 px-6">
                
                
                <div class="content">
                    <h1 id="convex-optimization">Convex Optimization</h1>
<p>A set $K\subseteq\mathbb{R}^n$ is called <strong>convex</strong> if for any $x,y\in K,\lambda\in[0,1]$ we have $\lambda x+(1-\lambda)y\in K$.</p>
<p>A function $f:K\to\mathbb{R}^n$ is called a convex function if $K$ is convex and for any $x,y\in K,\lambda\in[0,1]$ we have $\lambda f(x)+(1-\lambda)f(y)\ge f(\lambda x+(1-\lambda)y)$. <strong>In other words</strong>, the set $\lbrace(x,y)|y\ge f(x)\rbrace$ is a convex set in $\mathbb{R}^{n+1}$.</p>
<h2 id="supporting-hyperplane-and-subgradients">Supporting Hyperplane and Subgradients</h2>
<p><div class="rounded border" style="line-height:15px; background-color: #A7C1F2; border-radius: 5px; border-color: #AAAAAA; border-left-width: 2px; border-top-width: 2px;">
    <div class="px-2 pt-2 pb-2 font-bold">
        Seperating and Supporting Hyperplane Theorem
    </div>
    <div class="rounded border px-4 pt-2 pb-2" style="background-color: white; border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-color: #AAAAAA; border-top-right-radius: 0%; border-top-left-radius: 0%;">
        <div>
Suppose $K\subseteq\mathbb{R}^n$ is convex and closed. Then for any $y^*\in\mathbb{R}^n$ which is not a interior node of $K$, there exists $h\in\mathbb{R}^n\backslash\lbrace \mathbf{0}\rbrace$ such that for any $x\in K$ we have
<div>
\[\langle x,h\rangle\le\langle y^*,h\rangle\]
</div>
i.e., $h$ can define a hyperplane seperating $y^*$ and $K$.
</div>
    </div>
</div>
<div class="rounded border" style="background-color: white; border-left-width: 2px; border-top-width: 2px; border-color: #AAAAAA; ">
    <div class="px-2 pt-1 pb-1 font-bold"><a style="cursor:pointer" id="proofsupporting">Proof.</a></div>
    <div class="hidden px-4 pt-0 pb-2" id="proofcontentsupporting">
        <div>
Since $|x-y^* |^2$ is a continuous function on $x$ and $K$ is closed, there exists $x^* \in K$ with smallest distance to $y^*$. Suppose $y^*\not\in K$, then $h:=y^* -x^* \not=0$. We will prove that $\langle x-x^* ,h\rangle \le 0$ for any $x\in K$, which will lead to our conclusion since $0< \langle y^* -x^* ,h\rangle$. In the case $y^* \not\in K$, we have $\langle x,h\rangle<\langle y^* ,h\rangle$, which is stronger. 

Notice that $\lambda x +(1-\lambda)x^* \in K$ for any $\lambda\in[0,1]$, and $x^*$ has the smallest distance to $y^*$, which means
<div>
\[||\lambda x +(1-\lambda)x^*-y^* ||^2\ge ||x^* -y^* ||^2\]
\[||x^* -y^* +\lambda(x - x^*)||^2\ge ||x^* -y^* ||^2\]
\[-2\lambda\langle h,x-x^*\rangle +\lambda^2||x-x^*||^2\ge 0\]
</div>
By dividing $\lambda$ and take the limit $\lambda\to 0$, we get $\langle x-x^* , h\rangle\le 0$. 

Now if $y^* \in K$, then since $y$ is not a interior node, there is an infinite sequence $y_0,y_1,...\to y^* $, and we can assign each $y_i$ an $h_i$ according to the above discussion, satisfying $\langle x,h_i\rangle\le\langle y_i^* ,h_i\rangle$. w.o.l.g, we can let $||h_i||=1$. Now the sequence $h_0,h_1,h_2,...$ must have a convergence subsequence with limit point $h^*$, since they are bounded. By taking $i\to\infty$, we get $\langle x,h^* \rangle\le\langle y^* ,h^* \rangle$ for any $x\in K$. 
</div>
    </div>
</div>
<script>
    document.getElementById('proofsupporting').addEventListener('click', () => {
        element = document.getElementById('proofcontentsupporting')
        console.log(element.classList)
        if(element.classList.contains('hidden'))
            element.classList.remove('hidden')
        else
            element.classList.add('hidden')
    })
</script>
<div></div>
<div class="rounded border" style="line-height:15px; background-color: #A7C1F2; border-radius: 5px; border-color: #AAAAAA; border-left-width: 2px; border-top-width: 2px;">
    <div class="px-2 pt-2 pb-2 font-bold">
        Subgradients
    </div>
    <div class="rounded border px-4 pt-2 pb-2" style="background-color: white; border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-color: #AAAAAA; border-top-right-radius: 0%; border-top-left-radius: 0%;">
        <div>

Suppose $f:K\to\mathbb{R}$ is convex, then for any interior $x_0\in K$, there exists a vector $v$ such that for any $x\in K$ we have
<div>
\[f(x)\ge f(x_0)+\langle v,x-x_0\rangle\]
</div>
$v$ is called the subgradient. When $f$ is differentiable at $x_0$, then $v$ can only be $\nabla f(x_0)$. 
</div>
    </div>
</div>
<div class="rounded border" style="background-color: white; border-left-width: 2px; border-top-width: 2px; border-color: #AAAAAA; ">
    <div class="px-2 pt-1 pb-1 font-bold"><a style="cursor:pointer" id="proofsubgradients">Proof.</a></div>
    <div class="hidden px-4 pt-0 pb-2" id="proofcontentsubgradients">
        <div>
Recall that $\lbrace(x,y)|y\ge f(x)\rbrace$ is a convex set. By applying the above lemma on the set at $(x_0,f(x_0))$, we get a vector $h=\langle h_0,h_{n+1}\rangle $ where $h_0\in\mathbb{R}^n,h_{n+1}\in\mathbb{R}$. If $h_{n+1}=0$, then there exists $x-x_0$ with the same direction as $h_0$ since $x$ is an interior point. Letting $\langle x-x_0,h\rangle >0$, which is a contradiction. Thus, $h_{n+1}\not 0$.

We let $v=h_0/h_{n+1}$. Intuitively, $(x,v\cdot x)$ defines the plane perpendicular to $h$. Now we have
<div>
\[\langle h,(x,f(x))-(x_0,f(x_0))\rangle\le 0\]
\[\langle \frac{h_0}{h_{n+1}},x-x_0\rangle+\left(f(x)-f(x_0)\right)\le 0\]
</div>

By mulitply $-1$ at both side, we get our result. 
</div>
    </div>
</div>
<script>
    document.getElementById('proofsubgradients').addEventListener('click', () => {
        element = document.getElementById('proofcontentsubgradients')
        console.log(element.classList)
        if(element.classList.contains('hidden'))
            element.classList.remove('hidden')
        else
            element.classList.add('hidden')
    })
</script>
<div></div></p>
<p>The opposite direction of the above theorem also hold. So this is acctually an equivalent definition for convex function.
<div class="rounded border" style="line-height:15px; background-color: #A7C1F2; border-radius: 5px; border-color: #AAAAAA; border-left-width: 2px; border-top-width: 2px;">
    <div class="px-2 pt-2 pb-2 font-bold">
        Equivalence
    </div>
    <div class="rounded border px-4 pt-2 pb-2" style="background-color: white; border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-color: #AAAAAA; border-top-right-radius: 0%; border-top-left-radius: 0%;">
        <div>

Suppose $f:K\to\mathbb{R}$, then $f$ is convex iff. for any interior $x_0\in K$, there exists a vector $v$ such that for any $x\in K$ we have
<div>
\[f(x)\ge f(x_0)+\langle v,x-x_0\rangle\]
</div>
</div>
    </div>
</div>
For any $x,y\in K,\lambda\in(0,1)$, let $z=\lambda x+(1-\lambda y)$, we have</p>
<div>
\[\lambda f(x)\ge \lambda f(z)+\lambda\langle v,x-z\rangle\]
\[(1-\lambda)f(y)\ge (1-\lambda)f(z)+(1-\lambda)\langle v,y-z\rangle\]
</div>
Sum up this two inequalities. 
<h2 id="relation-to-derivative">Relation to Derivative</h2>
<p>When $f$ is diferentiable, the $v$ in the above discussion is simply $\nabla f$. We have the following relationship to derivative.
<div class="rounded border" style="line-height:15px; background-color: #A7C1F2; border-radius: 5px; border-color: #AAAAAA; border-left-width: 2px; border-top-width: 2px;">
    <div class="px-2 pt-2 pb-2 font-bold">
        Differential Convex Function
    </div>
    <div class="rounded border px-4 pt-2 pb-2" style="background-color: white; border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-color: #AAAAAA; border-top-right-radius: 0%; border-top-left-radius: 0%;">
        <div>
Suppose $f:K\to\mathbb{R}$ is twice differentiable. Then $f$ is convex if and only if for any $x,y\in K$ we have
<div>
\[\langle \nabla f(x)-\nabla f(y), x-y\rangle\ge 0\]
</div>
Which is also equivalent to
<div>
\[\nabla^2f\succeq 0\]
</div>

</div>
    </div>
</div></p>
<p>Proof uses the foundamental theorem in calculus in the mult-variable version.</p>
<h2 id="duality">Duality</h2>
<p>For an optimization problem of the following form (called <strong>Primal</strong>)</p>
<div>
\[\inf_{x\in\mathbb{R}^n}f(x)\]
\[f_i(x)\le 0\quad{\rm for }\ i\in[m]\]
\[h_j(x)=0\quad{\rm for }\ j\in[k]\]
</div>
<p>We can define the <strong>Lagrangian dual</strong> as the following convex optimization problem</p>
<div>
\[\sup_{\lambda\ge 0}g(\lambda)\]
\[g(\lambda,\mu):=\inf_{x\in\mathbb{R}^n}\left(f(x)+\sum_{i=1}^m\lambda_if_i(x)+\sum_{j=1}^k\mu_jh_j(x)\right)\]
</div>
<p>It is easy to see that the dual maximum value is at most the primal minumum value by swaping $\inf$ and $\sup$ in the dual problem (recall that minumum of maximums are at least the maximum of minimums). This is called the weak duality.</p>
<p>If we want the strong duality holds, i.e., dual solution is euqal to primal solution, a useful constraint to the primal problem is <a href="https://en.wikipedia.org/wiki/Slater%27s_condition">Slater&rsquo;s condition</a>. We have the following theorem.</p>
<div class="rounded border" style="line-height:15px; background-color: #A7C1F2; border-radius: 5px; border-color: #AAAAAA; border-left-width: 2px; border-top-width: 2px;">
    <div class="px-2 pt-2 pb-2 font-bold">
        Strong Duality
    </div>
    <div class="rounded border px-4 pt-2 pb-2" style="background-color: white; border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-color: #AAAAAA; border-top-right-radius: 0%; border-top-left-radius: 0%;">
        <div>
Suppose in Primal problem, $f,f_1,f_2,...,f_m$ are all convex functions, and $h_1,...,h_k$ are affine functions, and there exists $x^*\in\mathbb{R}^n$ such that $f_i(x^*)<0$ for all $i\in[m]$ and $h_j(x^* )=0$ for all $j\in[k]$, then the the dual and primal have the same solution.
</div>
    </div>
</div>
<div class="rounded border" style="background-color: white; border-left-width: 2px; border-top-width: 2px; border-color: #AAAAAA; ">
    <div class="px-2 pt-1 pb-1 font-bold"><a style="cursor:pointer" id="proofstrong dualitry">Proof.</a></div>
    <div class="hidden px-4 pt-0 pb-2" id="proofcontentstrong dualitry">
        <div>
Omit $h_j$ by decreasing the dimension. <span style="color:red">(need to verify)</span>
</br>
Consider the set $S=\{(y, w)\in\mathbb{R}^{n+1}| \exists x\in\mathbb{R}^ n, f(x)\le y\land f_i(x)\le w_i\}$. On can see that the set is convex. Suppose $y^* $ is the optimal value of primal, then $(y^* , 0)\in S$ and $y^* $ must not be the interior of $S$ (otherwise it is not optimal). And according to the hyperplane theorem we proved above, there exists $(\lambda_0,\lambda)\in\mathbb{R}^{n+1}$ such that
<div>
\[\forall (y,w)\in S, \lambda_0\cdot y+\lambda\cdot w\ge \lambda_0\cdot y^*\]
\[\Rightarrow\forall x\in\mathbb{R}^n, \lambda_0\cdot f(x)+\sum_{i=1}^m\lambda_i\cdot f_i(x)\ge \lambda_0\cdot y^*\]
</div>

Thus, if $\lambda_0>0$ then we are done: $g(\lambda/\lambda_0)$ is at least $y^* $. 

Use the Slater's condition, we can show that $\lambda_0\le 0$ is not possible: in which case we have 
<div>
\[\sum_{i=1}^m\lambda_i\cdot f_i(x^* )\ge 0\]
</div>
But there must exists an $\lambda_k\not=0$ since $(\lambda_0,\lambda)\not=\mathbf{0}$ and $\lambda_k\cdot f_k(x^* )< 0$, which is a contradiction.

</div>
    </div>
</div>
<script>
    document.getElementById('proofstrong dualitry').addEventListener('click', () => {
        element = document.getElementById('proofcontentstrong dualitry')
        console.log(element.classList)
        if(element.classList.contains('hidden'))
            element.classList.remove('hidden')
        else
            element.classList.add('hidden')
    })
</script>
<div></div>
<p><strong>Remark:</strong>  If we add affine equality constraint, then we can decrease the dimension and again check the condition and use the lemma.</p>
<p>The following lemma is useful to find the optimal value if functions are convext and differentiable, by just finding the stable point.</p>
<div class="rounded border" style="line-height:15px; background-color: #A7C1F2; border-radius: 5px; border-color: #AAAAAA; border-left-width: 2px; border-top-width: 2px;">
    <div class="px-2 pt-2 pb-2 font-bold">
        KKT condition
    </div>
    <div class="rounded border px-4 pt-2 pb-2" style="background-color: white; border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-color: #AAAAAA; border-top-right-radius: 0%; border-top-left-radius: 0%;">
        <div>
Let $L(x,\lambda,\mu)=f(x)+\sum_{i=1}^m\lambda_if_i(x)+\sum_{j=1}^k\mu_jh_j(x)$. Suppose $x'$ and $(\lambda',\mu')$ are the Primal and Dual optimal point and $f(x')=g(\lambda',\mu')$, then we must have $\partial L_x(x',\lambda',\mu')=0$ and $\lambda_i f_i(x)=0$ for all $i\in[m]$. If $f,f_i$ are convex and $h_j$ are affine, then the converse is also true.
</div>
    </div>
</div>
<p>The reverse condition give us $n+m+k$ equations while there are $n+m+k$ variables. Such an equation set might be easy to find a solution.</p>
<h2 id="mirror-decent">Mirror Decent</h2>
<p>Suppose we want to minimize a convex function $f:K\to\mathbb{R}$ where $K$ is a convex set. Consider starting at an intial point $x_0$ and update this point iteratively. Suppose we get point $x_t$, we use an approximation function to approximate $f$ near $x_0$:</p>
<div>
\[f(x_t)+\langle\nabla f(x_t),x-x_t\rangle+1/\eta\cdot D_{R}(x,x_t)\]
</div>
Where $D_R$ is the Bregman divergence of a function $R$ which is defined to be $D_R(x,y)=R(x)-R(y)-\langle\nabla R(y),x-y\rangle$. Note that if $R(x)=||x||^2_2$, then $D_R(x,y)=||x-y||^2_2$. 
<p>As $\eta$ gets smaller, the influence of $D_R(x,x_t)$ gets larger and the next step is more closer to $x_t$.</p>
<p>By finding the minimum value of this approximation we get the next step</p>
<div>
\[x_{t+1}=\argmin_{x\in K}\left(f(x_t)+\langle\nabla f(x_t),x-x_t\rangle+1/\eta\cdot D_{R}(x,x_t)\right)\]
\[=\argmin_{x\in K}\left(R(x)-\langle\nabla R(x_t)-\eta\nabla f(x_t),x\rangle\right)\]
</div>
<p>If we simply calculate the minimum point over $\mathbb{R}^n$, assume $R$ is convex and some good smooth condition, then we get</p>
<div>
\[w_{t+1}=\argmin_{x\in\mathbb{R}^n}\left(R(x)-\langle \nabla R(x_t)-\eta\nabla f(x_t),x\rangle\right)\]
\[\nabla R(w_{t+1})=\nabla R(x_t)-\eta\nabla f(x_t)\]
</div>
<p>What&rsquo;s interesting is that now $x_{t+1}$ becomes the projection of $w_{t+1}$ over the distance function $D_R$</p>
<div>
\[x_{t+1}=\argmin_{x\in K}\left(R(x)-\langle\nabla R(w_{t+1}),x\rangle\right)=\argmin_{x\in K}D_R(x,w_{t+1})\]
</div>
<p>The above discussion form the <strong>mirror decent</strong> algorithm, giving the oracle to compute derivative of $f$, the derivative and reverse derivative function for $R$, and the projection with regards to $D_R$.</p>
<h3 id="analysis">Analysis</h3>
<p>We will analyze the convergence of $\frac{1}{T}\sum_{i=0}^{T-1} x_i$. Assuming the optimal point of $f$ is $x^\star$, We have the following bound using the convexity of $f$</p>
<div>
\[f\left(\frac{1}{T}\sum_{i=0}^{T-1} x_i\right)-f(x^\star)\le \frac{1}{T}\sum_{i=0}^{T-1}\langle\nabla f(x_i),x_i-x^\star\rangle\]
</div>
<p>Now we write $g_i=\nabla f(x_i)$. The following analysis will not use the fact that $g_i$ is the gradient. i.e., the conclusion of the following analysis holds even for any given $g_i$, which leads to <strong>Multiplicative weights update</strong> that do not reley on computing the gradient of $f$.</p>
<p>Recall that we have $g_i=1/\eta\cdot\left(\nabla R(x_i)-\nabla R(w_{i+1})\right)$. Thus, we get</p>
<div>
\[\eta\langle\nabla g_i,x_i-x^\star\rangle=D_R(x^\star,x_i)+D_R(x_i,w_{i+1})-D_R(x^\star,w_{i+1}) \]
</div>
<p>The equality is the so-called <strong>Law of cosines for Bregman divergence</strong>. Now use the fact that $x_{i+1}$ lies between $w_{t+1}$ and any points in $K$, we have the following inequality</p>
<div>
\[\eta\langle\nabla g_i,x_i-x^\star\rangle\le D_R(x^\star,x_i)+D_R(x_i,w_{i+1})-\left(D_R(x^\star,x_{i+1})+D_R(x_{i+1},w_{i+1})\right) \]
</div>
<p>The inequality use the fact <strong>Pythagorean theorem for Bregman divergence</strong>. By telescoping we get</p>
<div>
\[\sum_{i=0}^{T-1}\eta\langle\nabla g_i,x_i-x^\star\rangle\le D_R(x^\star,x_0)+\sum_{i=0}^{T-1}\left(D_R(x_i,w_{i+1})-D_R(x_{i+1},w_{i+1})\right) \]
</div>
<p>Again use the Law of cosines for Bregman divergence, and recall that $R(x_{i})-\nabla R(w_{i+1})=g_i$, we get</p>
<div>
\[D_R(x_i,w_{i+1})-D_R(x_{i+1},w_{i+1})=\langle\eta g_i,x_{i}-x_{i+1}\rangle-D_R(x_{i+1},x_{i})\]
\[\le \eta\langle g_i,x_{i}-x_{i+1}\rangle-\sigma/2(||x_{i+1}-x_{i}||^*)^2\]
</div>
<p>where $||\cdot||$ and $||\cdot||^* $ are some dual norms under which $g_i$ and $R$ has some properties. The property for $R$ is that $R$ is $\sigma$-convex under the measure $||\cdot||^* $. For $g_i$ we consider two cases.</p>
<p>When $||g_i||$ is bounded by $G$. (if $g_i$ is $\nabla f(x_i)$ then that means $f$ have bounded gradient). Combining all the things together, and assuming the initial point has the property $D_R(x^\star,x_0)\le D$, we get</p>
<div>
\[\frac{1}{T}\sum_{i=0}^{T-1}\langle g_i,x_i-x^\star\rangle\le \frac{D}{T\eta}+\sum_{i=0}^{T-1}\left(G\cdot ||x_{i}-x_{i+1}||^*-\frac{\sigma}{2\eta}(||x_{i+1}-x_{i}||^*)^2\right)\]
\[\le \frac{D}{T\eta}+\frac{\eta G^2}{2\sigma}\le \sqrt{\frac{2DG^2}{T\sigma}}\]
</div>
<p>If we want to achieve $\epsilon$ approximation, then we need to choose $T=\frac{2DG^2}{\epsilon^2\sigma}$.</p>
<p>We sumarize all the requirement for mirror decent here.</p>
<div class="rounded border" style="line-height:15px; background-color: MediumSeaGreen; border-radius: 5px; border-color: #AAAAAA; border-left-width: 2px; border-top-width: 2px;">
    <div class="px-2 pt-2 pb-2 font-bold">
        Mirror Decent Algorithm
    </div>
    <div class="rounded border px-4 pt-2 pb-2" style="background-color: white; border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-color: #AAAAAA; border-top-right-radius: 0%; border-top-left-radius: 0%;">
        
Mirror decent to find the minimum of convex function $f$ should choose a good measurement functioin $R$ and the update rule is $x_{t+1}=\argmin_{x\in K}D_R(x,\nabla^{-1}R\left(\nabla R(x_t)-\eta g_t\right))$. We need the following assumption to compute the update.

<li>The first derivative of $f$ can be computed efficiently, so that we can let $g_t=\nabla f(x_t)$. (or instead, giving $g_i$ for any step in which case we don't need the gradient).</li>
<li>$\nabla R$ and $\nabla^{-1}R$ can be computed efficiently. </li>
<li>Projection $\argmin_{x\in K}D_R(x,\cdot)$ should be computed efficiently.</li>

    </div>
</div>
<div class="rounded border" style="line-height:15px; background-color: #A7C1F2; border-radius: 5px; border-color: #AAAAAA; border-left-width: 2px; border-top-width: 2px;">
    <div class="px-2 pt-2 pb-2 font-bold">
        Mirror Decent Efficiency
    </div>
    <div class="rounded border px-4 pt-2 pb-2" style="background-color: white; border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-color: #AAAAAA; border-top-right-radius: 0%; border-top-left-radius: 0%;">
        <div>
To make Mirror Decent efficient, we need to following assumptions. For some good choosed norm and dual norm $||\cdot||,||\cdot||^*$, if we have

<li>The $||\cdot||$ norm of the gradiant of $f$ is bounded by $G$.</li>

<li>$R$ is $\sigma$-strongly convex with respect to $||\cdot||^* $. In other words, we have $D_R(x,y)\ge\sigma/2(||x-y||^\star)^2 $</li>

<li>The distance between starting point and optimal point is guaranteed. i.e., we have $D_R(x^\star,x_0)\le D$.</li>

Then one can choose $\eta=\frac{\sigma\epsilon}{G^2}$ and runs for $T=\frac{2DG^2}{\epsilon^2\sigma}$ rounds to get $\epsilon$ approximation. 
</div>
    </div>
</div>
<p>Moreover, if we don&rsquo;t have bounded gradient but instead we have $L$-Lipschitz continuous gradient, then we can get the following bound</p>
<div>
\[\langle g_i,x_{i}-x_{i+1}\rangle\le L\cdot(||x_{i+1}-x_i||^\star)^2+f(x_{i})-f(x_{i+1})\]
\[\eta\langle g_i,x_{i}-x_{i+1}\rangle-\sigma/2(||x_{i+1}-x_{i}||^*)^2\le \left(\eta L-\frac{\sigma}{2}\right)(||x_{i+1}-x_i||^\star)^2+\eta\left(f(x_{i})-f(x_{i+1})\right)\]
</div>
<p>By letting $\eta=\frac{\sigma}{2L}$, we get the result</p>
<div>
\[\frac{1}{T}\sum_{i=0}^{T-1}\langle g_i,x_i-x^\star\rangle\le \frac{D}{T\eta}+\eta\sum_{i=0}^{T-1}(f(x_{i})-f(x_{i+1}))=\]
</div>
<h2 id="gradient-decent">Gradient Decent</h2>
<p>If we choose $R$ to be the simple $||\cdot||^2_2$, then $\nabla R(x)=2x$ and $\nabla^{-1} R(x)=x/2$, and $D_R(x,y)=||x-y||^2_2$. The step of mirror decent becomes</p>
<div>
\[w_{t+1}=x_t-\frac{\eta}{2}g_t\]
\[x_{t+1}=\argmin_{x\in K}||x-w_{t+1}||^2_2\]
</div>
<p>This is the classical gradient decent. Further, if we let $||\cdot||=||\cdot||^\star=||\cdot||_2$, then $R$ is clearly $2$-strongly convex. And if we have the $||\cdot||_2^2$ distance between starting point and optimal point is $D$, and assume the gradient of $f$ is bounded by $G$, then according to the efficiency of mirror decent, the number of rounds to converge is guaranteed.</p>
<p>We discuss another analysis here. We do not assume bounded gradient, but insteat we assume $f$ have L-Lipschitz continuous gradient. For simplisity we assume $K=\mathbb{R}^b$ and $\eta=\eta/2$. And further we want to prove the convergence of $f(x_i)$. Let $f(x^\star)$ be the optimal value, and let $R_i=f(x_i)-f(x^\star)$, we will prove the convergence of $R_i$.</p>
<div>
\[R_i-R_{i+1}=f(x_i)-f(x_{i+1})\ge -\langle \nabla f(x_i),x_{i+1}-x_{i}\rangle-\frac{L}{2}||x_{i+1}-x_i||^2_2\]
\[=(\eta- \eta^2L/2)g_i^2=\frac{g_i^2}{2L}\]
</div>
<p>The next step choose $\eta=1/L$. We now relates $g_i$ to $R_i$.</p>
<div>
\[R_i=f(x_i)-f(x^\star)\le \langle g_i,x^\star-x_i\rangle\le D||g_i||_2\]
</div>
<p>Where we assume another condition $D$ is an upper bound for $||x^\star-x_0||_2$. Now each time $R_i$ decrease for at least $\frac{R_i^2}{2D^2L}$. Thus, to decrease to half of $R_i$, it needs $\lceil\frac{4LD^2}{R_i}\rceil$ times. The total time to converge to less then $\epsilon$ can be computed as</p>
<div>
\[\sum_{i=1}^{2^i\epsilon=R_0}\lceil\frac{4LD^2}{2^i\epsilon}\rceil\le \log R_0/\epsilon+\frac{4LD^2}{\epsilon}\]
</div>
<p>Here $R_0$ can be bounded by $||g_0||_2\cdot ||x_0-x^\star||_2$. Note that $\nabla f(x^\star)=0$, thus $||g_0||_2\le L/2||x_0-x^\star||_2$. In any case, $\log R/\epsilon$ is not comparable to $D/\epsilon$, thus, the convergent number of rounds is $\frac{4LD^2}{\epsilon}.$</p>
<h2 id="accelerated-gradient-descent">Accelerated Gradient Descent</h2>
<p>Accelerated Gradient Descent is a combinition of Gradient decent and Mirror decent. This will lead to a $\sqrt{\epsilon}^{-1}$ time cost, much faster than previous algorithm. Starting from a point $x_0,y_0,z_0$, we procede the decent as following: $z_{t-1}$ procedes a mirror decent based on the gradient at $y_{t-1}$ to get $z_t$, $y_{t-1}$ procedes a gradient decent to get $x_t$, choose an appropriete linear combinition of $x_t$ and $z_t$ to get $y_t$.</p>
<p>In this section $||\cdot||$ is an arbitrary norm.
<div class="rounded border" style="line-height:15px; background-color: MediumSeaGreen; border-radius: 5px; border-color: #AAAAAA; border-left-width: 2px; border-top-width: 2px;">
    <div class="px-2 pt-2 pb-2 font-bold">
        Accelerated Gradient Descent Algorithm
    </div>
    <div class="rounded border px-4 pt-2 pb-2" style="background-color: white; border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-color: #AAAAAA; border-top-right-radius: 0%; border-top-left-radius: 0%;">
        
<div>
Initialize $x_0,z_0$, choose a regularizer $R$, choose appropriate $\gamma_t,\eta_t,\eta'_t$ for any $t\in\mathbb{N}^+$ do
\[y_{t-1}=(1-\gamma_t)x_{t-1}+\gamma_t z_{t-1}\]
\[z_t=\argmin_{x}\left(\eta_t\langle \nabla f(y_{t-1}),x\rangle +D_R(x,z_{t-1})\right)=\nabla^{-1}R\left(\nabla R(z_{t-1})-\eta_t\nabla f(y_{t-1})\right)\]
\[x_t=\argmin_{x}\left(\eta'_t\langle \nabla f(y_{t-1}),x\rangle +\frac{1}{2}||x-y_{t-1}||^2\right)\]
i.e., when $||\cdot||=||\cdot||_2$, the algorithm is
\[z_t=\nabla^{-1}R\left(\nabla R(z_{t-1})-\eta_t\nabla f(y_{t-1})\right)\]
\[x_t=y_{t-1}-\eta'_t\nabla f(y_{t-1})\]
\[y_t=(1-\gamma_t)x_t+\gamma_t z_t\]
</div>

    </div>
</div></p>
<p>With some assumptions we can prove the efficiency.
<div class="rounded border" style="line-height:15px; background-color: #A7C1F2; border-radius: 5px; border-color: #AAAAAA; border-left-width: 2px; border-top-width: 2px;">
    <div class="px-2 pt-2 pb-2 font-bold">
        Accelerated Gradient Descent Efficiency
    </div>
    <div class="rounded border px-4 pt-2 pb-2" style="background-color: white; border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-color: #AAAAAA; border-top-right-radius: 0%; border-top-left-radius: 0%;">
        <div>
If the following conditions are satisfied
<li>$f$ is $1$-smooth with repect to norm $||\cdot||$. </li>
<li>$R$ to be $1$-strongly convex with respect to norm $||\cdot||$. </li>
<li>$D_R(z_0=x_0,x^\star)$ is bounded by $D^2$ where $x^\star$ is the opbimal point for $f$. </li>
then after $t$ rounds, we have the approximation
<div>
\[f(x_t)-f(x^\star)\le O\left(\frac{D^2}{t^2}\right)\]
</div>
</div>
    </div>
</div></p>
<p>i.e., to get $\epsilon$ approximation, we need $\frac{D}{\epsilon}$ rounds.</p>
<p>If $f$ is $L$-Lipschitz and $R$ is $\sigma$-strongly convex, then run the algorithm on $f/L$ and $R/\sigma$, which fit the algorithm requirements. The reason is the observation $D_{f/L}(x,y)=D_f(x,y)/L$, $D_{R/\sigma}(x,y)=D_R(x,y)/\sigma$. Now the condition $D_R(x_0,x^\star)\le D^2$ becomes $D_{R/\sigma}(x_0,x^\star)\le D^2/\sigma$. And if we want the function to converge to $\epsilon$, we need $f/L$ to converge to $\epsilon/L$. Thus, we need $O\left(\frac{LD^2}{\sigma\epsilon}\right)$ rounds.</p>
<h3 id="analysis-1">Analysis</h3>
<p>To analize the algorithm and choose appropriate parameter, the key idea is to maintain a function $\phi_{t}$ to approximate $f$. $\phi_t$ is defined recursively as following</p>
<div>
\[\phi_0=f(z_0)+D_R(x,z_0)\]
\[\phi_t=(1-\gamma_t)\phi_{t-1}+\gamma_t\left(f(y_{t-1})+\langle x-y_{t-1},\nabla f(y_{t-1})\rangle\right)\]
</div>
<p>i.e., $\phi_t$ is a linear combinition of $\phi_{t-1}$ and a linear function which is the tangent lower bound for the convex function $f$ at $y_{t-1}$. Such a combinition will gradually drop $\phi_t$ down to $f$. We will prove that it satisfies the following lower and upper bound properties.</p>
<ul>
<li>$(1-\lambda_t)f+ \lambda_t\phi_0\ge\phi_t$ where $\lambda_t=\left(\prod_{i=1}^t(1-\gamma_i)\right)$.</li>
<li>$f(x_t)\le\phi_t(x)$ for any $x$.</li>
</ul>
<p>If we have the above two properties, then we immediately get</p>
<div>
\[f(x_t)\le\phi_t(x^\star)\le(1-\lambda_t)f(x^\star)+\lambda_t\phi_0(x^\star)\le f(x^\star)+\lambda_t\left(f(z_0)-f(x^\star)+D_R(x^\star,z_0)\right)\le f(x^\star)+2\lambda_t D^2\]
</div>
<p>Now we prove the two properties. The first properties is obvious by induction. We focus on the second one. The idea is to use induction, and try to set $y_t,x_t$ to make $\phi_t(x)$ becomes some type of second expansion approximation of $f$.</p>
<p>We first claim that $\phi_t$ is a Bregmen divergence based on $z_t$: $\phi_t(x)=\phi_t(z_t)+\lambda_t D_R(x,z_t)$. That can be done by direction and the fact <strong>Shifting Bregman divergence by a linear term</strong>.</p>
<div>
\[\phi_t(x)=(1-\gamma_{t})\phi_{t-1}(x)+\gamma_t(f(y_{t-1})+\langle x-y_{t-1},\nabla f(y_{t-1})\rangle)\]
\[=(1-\gamma_{t})(\phi_t(z_{t-1})+\lambda_{t-1} D_R(x,z_{t-1}))+\gamma_t(f(y_{t-1})+\langle x-y_{t-1},\nabla f(y_{t-1})\rangle)\]
\[=\lambda_t D_R(x,z_{t-1})+\gamma_t\langle \nabla f(y_{t-1}),x\rangle +C\]
\[=\lambda_t \left(D_R(x,z_{t-1})+\langle\frac{\gamma_t}{\lambda_t} \nabla f(y_{t-1}),x\rangle\right) +C\]
</div>
<p>where the shifting of Bregman divergence</p>
<div>
\[D_R(x,z_{t-1})+\langle\frac{\gamma_t}{\lambda_t} \nabla f(y_{t-1}),x\rangle=R(x)-\langle\nabla R(z_{t-1})-\frac{\gamma_t}{\lambda_t} \nabla f(y_{t-1}),x\rangle+C=D_R(x,z_t)+C'\]
</div>
<p>Where we set $\nabla R(z_t)=\nabla R(z_{t-1})-\frac{\gamma_t}{\lambda_t} \nabla f(y_{t-1})$. Thus, we have</p>
<div>
\[\phi_t(x)=\lambda_t D_R(x,z_t)+C=\lambda_t D_R(x,z_t)+\phi_t(z_t)\]
</div>
Now we use the fact to find $x_t$ such that $f(x_t)\le\phi_t(x)$ by induction.
<div>
\[\phi_t(x)=(1-\gamma_{t})\phi_{t-1}(x)+\gamma_t(f(y_{t-1})+\langle x-y_{t-1},\nabla f(y_{t-1})\rangle)\]
\[=(1-\gamma_{t})(\phi_t(z_{t-1})+\lambda_{t-1}D_R(x,z_t))+\gamma_t(f(y_{t-1})+\langle x-y_{t-1},\nabla f(y_{t-1})\rangle)\]
\[\ge(1-\gamma_{t})(f(x_{t-1})+\lambda_{t-1}D_R(x,z_{t-1}))+\gamma_t(f(y_{t-1})+\langle x-y_{t-1},\nabla f(y_{t-1})\rangle)\]
\[\ge (1-\gamma_t)(f(y_{t-1})+\langle\nabla f(y_{t-1}),x_{t-1}-y_{t-1}\rangle)+\gamma_t(f(y_{t-1})+\langle x-y_{t-1},\nabla f(y_{t-1})\rangle)+\lambda_tD_R(x,z_{t-1})\]
\[\ge f(y_{t-1})+\langle\nabla f(y_{t-1}),\gamma_tx+(1-\gamma_t) x_{t-1}-y_{t-1}\rangle +\frac{\lambda_t}{2}||x-z_{t-1}||^2\]
</div>
<p>We let $y_{t-1}=(1-\gamma_t) x_{t-1}+\gamma_t z_{t-1}$, and let $\tilde{x}-y_{t-1}=\gamma_t(x-z_{t-1})$, then we get</p>
<div>
\[\phi_t(x)\ge f(y_{t-1})+\langle\nabla f(y_{t-1}),\tilde{x}-y_{t-1}\rangle+\frac{\lambda_t}{2\gamma_t^2}||\tilde{x}-y_{t-1}||^2\]
</div>
<p>We let $\frac{\lambda_t}{\gamma_t^2}\ge 1$, and set $x_t=\argmin_{\tilde{x}}\left(\langle\nabla f(y_{t-1}),\tilde{x}\rangle+\frac{1}{2}||\tilde{x}-y_{t-1}||^2\right)$, then for any $x$ we have</p>
<div>
\[\phi_t(x)\ge f(y_{t-1})+\langle\nabla f(y_{t-1}),x_t-y_{t-1}\rangle+\frac{1}{2}||x_t-y_{t-1}||^2\ge f(x_t)\]
</div>
<p>Thus, we finished. One can choose $\gamma_t=\frac{2}{t+3}$ to make $\lambda_t=\frac{6}{(t+2)(t+3)}\ge\gamma_t^2$. Finally, we get</p>
<div>
\[f(x_t)\le f(x^\star)+O\left(\frac{D^2}{t^2}\right)\]
</div>
<p>If $f$ is $L$-Lipschitz and $R$ is $\sigma$-strongly convex, then run the algorithm on $f/L$ and $R/\sigma$, which fit the algorithm requirements. The reason is the observation $D_{f/L}(x,y)=D_f(x,y)/L$, same for $R$. Now the condition $D_R(x_0,x^\star)\le D^2$ becomes $D_{R/\sigma}(x_0,x^\star)\le D^2/\sigma$. And if we want the function to converge to $\epsilon$, we need $f/L$ to converge to $\epsilon/L$. Thus, we need $O\left(\frac{LD^2}{\sigma\epsilon}\right)$ rounds.</p>
<h3 id="when-function-is-both-lipschitz-and-strongly-convex">When Function is both Lipschitz and Strongly Convex</h3>
<p>Suppose we want to minimize a convex function $f$ which is $L$-Lipschitz and $\beta$-strongly convex, where $L\ge\beta$. Consider the following procedure: choose $R=||\cdot||_2$, start an accelerated decent where $||x_0-x^\star||_2^2\le D^2$. According to the previous discussion, after $t$ rounds, $f(x_t)$ becomes $O(LD^2/t^2)$ close to the optimal value. But notice that $f$ is $\sigma$-strongly convex, which means the change of function value is a good upper bound for the change of variable, i.e., we have</p>
<div>
\[f(x^\star)+\frac{\beta}{2}||x^\star-x_t||_2^2\le f(x_t)\]
\[||x^\star-x_t||_2^2\le \frac{2}{\beta}\left(f(x_t)-f(x^\star)\right)\le O\left(\frac{LD^2}{\beta t^2}\right)\]
</div>
<p>i.e., after $O(\sqrt{L/\beta})$ loops, the bound for $||x^\star-x_t||_2^2$ decrease by a constant factor. Thus, just consider setting a new starting point $x_0&rsquo;$ at $x_t$ and run a new accelerated decent (the approximate efficiency $\gamma_t=O(1/t^2)$ decreases fastly as $t$ grows, so its reasonable to restart at some time).</p>
<p>Now to decrease $O(LD^2)$ to $\epsilon$, we need $\log\left(LD^2/2\epsilon\right)$ times of restart, which leads to running time</p>
<div>
\[O\left(\sqrt{\frac{L}{\beta}}\cdot\log\left(\frac{LD^2}{\epsilon}\right)\right)\]
</div>
<h3 id="application-solve-linear-system">Application: Solve Linear System</h3>
<p>Suppose we are giving a reversible matrix $A$ and a vector $b$, and want to solve the equation $Ax=b$. Recall that $A^TA$ is symmetric and since $(Ax)^TAx\not=$ for any $x\not=0$ since $A$ is reversible, we know all $A^TA$ is positive semi-definite and all $n$ real eigenvalues are greater then $0$. We use $\lambda_n(A^TA)$ and $\lambda_1(A^TA)$ to denote the largest and smallest eigenvalue of $A^TA$, and write $\kappa(A^TA)=\frac{\lambda_n(A^TA)}{\lambda_1(A^TA)}$.</p>
<p>Now we want to optimize the convex function $f(x)=\frac{1}{2}||Ax-b||_2^2$. We have $\nabla^2 f(x)=A^TA$. Notice that $\lambda_1(A^TA)I\preceq A^TA\preceq\lambda_n(A^TA)$, so we know $\nabla^2 f(x)$ is $\lambda_n(A^TA)$-Lipschisz and $\lambda_1(A^TA)$-strongly convex. By previous discuss, there is an algorithm finding an $x$ such that $\frac{1}{2}||Ax-b||^2_2\le\epsilon$ in</p>
<div>
\[O\left(\kappa(A^TA)\cdot\log\left(\frac{\lambda_n(A^TA)||x^\star||_2^2}{\epsilon}\right)\right)\]
</div>
<p>times of gradient computation, which is $n^2$ matrix multiplied by vector, where $x^\star$ is $A^{-1}b$, as we assume we start with $x_0=0$.</p>
<h2 id="newtons-method">Newton&rsquo;s Method</h2>
<p>Newton&rsquo;s method can be used to find the zero point of a funcion, which leads to the minimum value of a convex function if we find the zero point of its derivative.</p>
<h3 id="intuition-1">Intuition 1</h3>
<p>One intuition for Newton&rsquo;s Method is the analog of the one dimantional case: find the tangent of the curve $g(x)$ at $x_0$ and let the intersection of the tangent and $x$-axis be the next point $x_1$. This method leads to the iteration $x_1=x_0-g(x_0)/g&rsquo;(x_0)$. It is not hard to imagine that in multi-variable case we have the iteration</p>
<div>
\[x_1=x_0-(\nabla g(x_0))^{-1}g(x_0)\]
</div>
<p>Where $g$ is a function from $\mathbb{R}^n$ to $\mathbb{R}^n$, and $\nabla g$ is an $n\times n$ invertible matrix. That is reasonable since we will use $g$ as the gradiant of the convex function we want to optimize.</p>
<h3 id="intuition-2">Intuition 2</h3>
<p>Surprisingly, if we use the same intuition for Mirror or gradiant decent (find the minimum of an approximation function around $x_0$), we essentially get the same iteration equation. Recall that in mirror decent we approximate $f$ at $x_0$ as $f(x_0)+\langle\nabla f(x_0),x-x_0\rangle+D_R(x,x_0)$ where $D_R$ measure the distance between the next point and $x_0$; also recall that in gradient decent we set the distance as the trivial Euclidean distance. However, it is well known that the (possiblely) best approximation is the taylor expansion where we acctually multiplied the Euclidean distance by the function&rsquo;s second derivative.</p>
<div>
\[f(x_0)+\nabla f(x_0)(x-x_0)+\frac{1}{2}(x-x_0)^T\nabla^2f(x_0)(x-x_0)\]
</div>
<p>If we find the minimum of this function, we also get the Newton&rsquo;s Method</p>
<div>
\[x_1=x_0-(\nabla^2f(x_0))^{-1}\nabla f(x_0)\]
</div>
<h3 id="ne-condition">NE condition</h3>
<p>Now we seek for assumptions that make Newton&rsquo;s Method efficient. Use standard analysis in one-dimation we can see the crutial value for the convergence is $g&rsquo;&rsquo;(x)/g&rsquo;(x)$, the smaller the better.</p>
<div class="rounded border" style="line-height:15px; background-color: #A7C1F2; border-radius: 5px; border-color: #AAAAAA; border-left-width: 2px; border-top-width: 2px;">
    <div class="px-2 pt-2 pb-2 font-bold">
        Convergence in NE condition
    </div>
    <div class="rounded border px-4 pt-2 pb-2" style="background-color: white; border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-color: #AAAAAA; border-top-right-radius: 0%; border-top-left-radius: 0%;">
        <div>
Let $f:\mathbb{R}^n\to\mathbb{R}$ be a function and write $H(x)=\nabla^2f(x)$ for simplicity. Let $x^\star$ be one of its minimal value. Suppose there exists $L,h,R$ such that in the Euclidean Ball centered as $x^\star$ with radio $R$ denoted as $B(x^\star, R)$, has the following property (called $NE\left(\frac{L}{2h}\right))$ condition)
<ul>
<li>
<p>For any $x\in B(x^\star, R)$, the spectrum norm of $H(x)^{-1}$ is at most $\frac{1}{h}$.</p>
</li>
<li>
<p>For any $x,y\in B(x^\star, R)$, the spetrum norm of $H(x)-H(y)$ is at most $L||x-y||_2$.</p>
</li>
</ul>
<p>Then by letting $x_1=x_0-(\nabla^2f(x_0))^{-1}\nabla f(x_0)$ for any $x_0\in B(x^\star,R)$, we have</p>
<div>
\[||x_1-x^\star||_2\le \frac{L}{2h}\cdot ||x_0-x^\star||_2\]
</div>
</div>
    </div>
</div>
<h3 id="intuition-3">Intuition 3</h3>
<p>Notice that the term &ldquo;Gradient decent&rdquo; originally comes from the dream to take the best direction towards smaller value, i.e., it seek for a vector $e$ such that $||e||=1$ and the directional derivative $\nabla f(x_0)[e]=\nabla f(x_0)\cdot e$ is minimum. When $||\cdot||$ is the Euclidean norm, $e$ should be in the same direction as $-\nabla f(x_0)$ to make it minimum according to Cauchy–Schwarz inequality.</p>
<p>The interesting thing is that if we change the norm $||\cdot||$ in an appropriate way, we can acctually get Newton&rsquo;s Method: just write $\nabla f(x_0)\cdot e$ as $\nabla f(x_0)(\nabla^2f(x_0))^{-\frac{1}{2}}\cdot (\nabla^2f(x_0))^{\frac{1}{2}}e$ and use Cauchy–Schwarz inequality on this two vectors, which leads to</p>
<div>
\[e=(\nabla^2f(x_0))^{-\frac{1}{2}}\cdot \left(\nabla f(x_0)(\nabla^2f(x_0))^{-\frac{1}{2}}\right)^T=\left(\nabla^2 f(x_0)\right)^{-1}\nabla f(x_0)\]
</div>
<p>And that means we should use the assumption $e^T\nabla^2 f(x_0)e=1$. So we set the norm $||\cdot||=\sqrt{e^T\nabla^2 f(x_0)e}$. That is called the <strong>Local Norm</strong> based on $x_0$ in the book, written as $||\cdot||_{x_0}$. It will lead to the intuition for the following condition and analysis.</p>
<h3 id="nl-condition">NL condition</h3>
<p>Remember that $g&rsquo;&rsquo;(x)/g&rsquo;(x)$ is crucial, which can also be written as $\left|\frac{g&rsquo;(x)-g&rsquo;(y)}{x-y}\cdot\frac{1}{g&rsquo;(x)}\right|$. If this term is bounded by a constant $M$, where $|x-y|=\delta$ is small enough, then $(1-M\delta)g&rsquo;(x)\le g&rsquo;(y)\le (1+M\delta)g&rsquo;(x)$, whenever $x$ and $y$ are close. Combining the local norm, we get the following lemma.</p>
<div class="rounded border" style="line-height:15px; background-color: #A7C1F2; border-radius: 5px; border-color: #AAAAAA; border-left-width: 2px; border-top-width: 2px;">
    <div class="px-2 pt-2 pb-2 font-bold">
        Convergence in NL condition
    </div>
    <div class="rounded border px-4 pt-2 pb-2" style="background-color: white; border-left-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-color: #AAAAAA; border-top-right-radius: 0%; border-top-left-radius: 0%;">
        <div>
Let $f:\mathbb{R}^n\to\mathbb{R}$ be a function and write $H(x)=\nabla^2f(x)$ for simplicity. Let $x^\star$ be one of its minimal value. Suppose for any $x,y\in\mathbb{R}$ where $||x-y||_{x}\le\delta\le\frac{1}{6}$ we have
<div>
\[(1-3\delta)\nabla^2f(x)\le\nabla^2f(y)\le(1+3\delta)\nabla^2f(x)\]
</div>
<p>Then by letting $x_1=x_0-n(x_0)$ where $n(x_0)=(\nabla^2f(x_0))^{-1}\nabla f(x_0)$ and $||n(x_0)||_{x_0}\le\frac{1}{6}$, then we have</p>
<div>
\[||n(x_1)||_{x_1}\le 3||n(x_0)||^2_{x_0}\]
</div>
</div>
    </div>
</div>
<h2 id="interior-node-method">Interior Node Method</h2>
<p>We consider the linear programming problem that given $c,b,A$, minimize $c^Tx$ under the constraint area $P={ x\in\mathbb{R}^n\mid Ax\le b }$. Such an optimization problem is constrained, which means if we apply constrained decent algorithm above, we will do the proceeding-projecting method. Such method is not efficient: it is acctually similar to the simplex algorithm, which is proved to be exponential in the worst case.</p>
<p>Thus, we seek for a formulation such that we donot need to do projection. That means, our goal function is supposed to be defined and continuous among $\mathbb{R}^n$, where each proceeding is guaranteed to fall in the constraint area. A natural idea is to make the value outside the constraint area be $+\infty$, and the boundary of the area tends to $+\infty$. Such a function can be continuous and even smooth, but the problem is that such a function cannot be either bounded gradiant or Lipschitz - the small area near the boundary has gradients tends to $+\infty$.</p>
<p>Fortunately, the NL condition for Newton&rsquo;s method do not need bounded gradient. Actually, the local norm will be also huge when the twice gradient is huge. One is possible to come up with a function satisfying NL condition. A straight forward way is to use $\log$ to serve the above purpose. We assume the i-th row of $A$ is $A_i$, then</p>
<div>
\[F(x)=\left\{
\begin{aligned}
&-\sum_{i}\log(b_i-A_ix) & & x\in P \\
&+\infty & & x\not\in P \\
\end{aligned}
\right.
\]
</div>
<p>Here we define $\log(0)=-\infty$. Such a function is smooth. We can also prove <strong>it satisfies the NL condition</strong>.</p>
<div class="rounded border" style="background-color: white; border-left-width: 2px; border-top-width: 2px; border-color: #AAAAAA; ">
    <div class="px-2 pt-1 pb-1 font-bold"><a style="cursor:pointer" id="proofsatisfies the NL condition">Proof.</a></div>
    <div class="hidden px-4 pt-0 pb-2" id="proofcontentsatisfies the NL condition">
        <div>
<p>The Hessian of $F$ is</p>
<div>
\[H_F(x)=\sum_k\frac{A_k^TA_k}{(b_k-A_kx)^2}\]
</div> 
<p>Thus, we focus on estimating $S_k(k)=b_k-A_kx$ by changing $x$ to $y$, where we have $||x-y||_x\le\delta\le\frac{1}{6}$, which lead to</p>
<div>
\[\sum_k\frac{|(x-y)A_k|^2}{S_k(x)^2}\le\delta^2\]
</div> 
<p>The above inequality implies each term of the lesf hand side is at most $\delta^2$ since each term is non-negtive. Thus, for any $k$ we have</p>
<div>
\[\frac{|S_k(y)-S_k(x)|}{|S_k(x)|}\le\delta\]
\[(1-\delta)S_k(x)\le S_k(y)\le(1+\delta)S_k(x)\]
\[(1-3\delta)\frac{1}{S_k(x)^2}\le (1+\delta)^{-2}\frac{1}{S_k(x)^2}\le \frac{1}{S_k(y)^2}\le(1-\delta)^{-2}\frac{1}{S_k(x)^2}\le(1+3\delta)\frac{1}{S_k(x)^2}\]
</div> 
</div>
    </div>
</div>
<script>
    document.getElementById('proofsatisfies the NL condition').addEventListener('click', () => {
        element = document.getElementById('proofcontentsatisfies the NL condition')
        console.log(element.classList)
        if(element.classList.contains('hidden'))
            element.classList.remove('hidden')
        else
            element.classList.add('hidden')
    })
</script>
<div></div>
<p>But remember that we want to minimize $c^Tx$, which is a linear function has has $0$ Hessian. Thus, we intergrate $c^Tx$ to our $F(x)$ with weight $\eta$.</p>
<div>
\[f_\eta(x)=\eta c^Tx+F(x)\]
</div>
<p>$f_\eta(x)$ has the same Hessian as $F(x)$. Thus, by applying Newton&rsquo;s method on $f_\eta(x)$, we can easily converge to the minimum of $f_\eta(x)$, denoted as $x^\star_\eta$. As long as $\eta\to+\infty$, $x^\star_\eta$ tends to the minimum of $\eta$. The trace of $x^\star_\eta$ as $\eta\to+\infty$ is called the center path, lies in the interior of $P$. That&rsquo;s why the method is called interior point method.</p>
<p>However, remember that Newton&rsquo;s Method only work when the Newton step $n _\eta(x)=(H_F(x))^{-1}\nabla f _\eta(x)$ is bounded. It&rsquo;s not easy to find an intial point satisfying this condition for large $\eta$. Thus, we consider finding an intial point with small $\eta$ and then update $\eta$ by following the center path.
Formally, <strong>we will prove $||n _\eta(x)||_x$ won&rsquo;t vary too much by increase $\eta$ by a factor</strong>.</p>
<div class="rounded border" style="background-color: white; border-left-width: 2px; border-top-width: 2px; border-color: #AAAAAA; ">
    <div class="px-2 pt-1 pb-1 font-bold"><a style="cursor:pointer" id="proofvery eta">Proof.</a></div>
    <div class="hidden px-4 pt-0 pb-2" id="proofcontentvery eta">
        <div>
We want to bound $||n_{\eta'}(x)||_x$ for $\eta'>\approx \eta$. For simplicity we write $H(x)=H_F(x)$. 
<div>
\[n_{\eta'}(x)=(H(x))^{-1}(\eta'c^T+\nabla F(x))\]
\[=\frac{\eta'}{\eta}(H(x))^{-1}(\eta c^T+\nabla F(x))+\left(1-\frac{\eta}{\eta'}\right)(H(x))^{-1}\nabla F(x)\]
</div>

The first term is $\frac{\eta'}{\eta}n_\eta(x)$, which is good. Our task is to bound the second term, i.e., the size of $z(x)=(H(x))^{-1}\nabla F(x)$ is not too large. First we use triangle inequality to relates the sizes of them.

<div>
\[||n_{\eta'}(x)||_x\le \frac{\eta'}{\eta}||n_\eta(x)||_x+\left(1-\frac{\eta}{\eta'}\right)||z(x)||_x\]
</div>

Then we focus on $||z(x)||_x$. We will use the nice property of $\nabla f$ and $H(x)$: the squre of $\nabla f$ will lead us to $H(x), which is key to the following proof.

<div>
\[||z(x)||_x^2=\nabla f(x)^Tz(x)=\sum_{k}\frac{a_kz(x)}{s_k(x)}\le\sqrt{m\cdot z(x)^T\sum_k\frac{a_k^Ta_k}{s_k(x)}z(x)}=\sqrt{m}||z_(x)||_x\]
</div>

Thus, we get $||z(x)||^2_x\le\sqrt{m}$. Finally we have

<div>
\[||n_{\eta'}(x)||_x\le\frac{\eta'}{\eta}||n_{\eta}(x)||_x+\left(1-\frac{\eta}{\eta'}\right)\sqrt{m}\]
</div>

</div>
    </div>
</div>
<script>
    document.getElementById('proofvery eta').addEventListener('click', () => {
        element = document.getElementById('proofcontentvery eta')
        console.log(element.classList)
        if(element.classList.contains('hidden'))
            element.classList.remove('hidden')
        else
            element.classList.add('hidden')
    })
</script>
<div></div>
<p>If we let $\eta&rsquo;=(1+\Omega(1/\sqrt{m}))\eta$, then we can bound $||n_{\eta&rsquo;}(x)||<em>x$ by $\epsilon||n</em>{\eta&rsquo;}(x)||_x+\epsilon$, where $\epsilon$ is a small contant which we can set as small as we want. Moreover, $\eta$ grows exponentially, on the base of $\sqrt{m}$.</p>
<p>Thus, by starting from an intial point, and repeated perform: constant times of Newton&rsquo;s method to let $||n_\eta(x)||<em>x$ be a sufficiently small constant 2. change $\eta$ to let $||n</em>{\eta&rsquo;}(x)||<em>x$ still lie in the Newton&rsquo;s Method efficency area. There are two more things to show: 1. how to find the initial point. We will do this later. 2. At some point we will get sufficiently large $\eta$ and a small constant bound for $||n</em>{\eta&rsquo;}(x)||_x$, how could this lead to $c^Tx$ close to $c^Tx^\star$? We will first show the later.</p>
<div class="rounded border" style="background-color: white; border-left-width: 2px; border-top-width: 2px; border-color: #AAAAAA; ">
    <div class="px-2 pt-1 pb-1 font-bold"><a style="cursor:pointer" id="prooffinal step">Proof.</a></div>
    <div class="hidden px-4 pt-0 pb-2" id="proofcontentfinal step">
        <div>
I don't understand...
</div>
    </div>
</div>
<script>
    document.getElementById('prooffinal step').addEventListener('click', () => {
        element = document.getElementById('proofcontentfinal step')
        console.log(element.classList)
        if(element.classList.contains('hidden'))
            element.classList.remove('hidden')
        else
            element.classList.add('hidden')
    })
</script>
<div></div>
<p>Finding the initial point: TBD.</p>
<p>Too hard for me&hellip;</p>

                </div>
                
                

                



                



            </div>
            
            <div class="hidden lg:block lg:w-1/4">
                
                <div class="sticky top-16 z-10 hidden lg:block px-6 py-4  bg-secondary-bg pt-16 -mt-16 ">
    <span class="text-lg font-semibold">On This Page</span>
</div>
<div class="sticky-toc hidden lg:block px-6 pb-6  pt-10 -mt-10 border-l ">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#convex-optimization">Convex Optimization</a>
      <ul>
        <li><a href="#supporting-hyperplane-and-subgradients">Supporting Hyperplane and Subgradients</a></li>
        <li><a href="#relation-to-derivative">Relation to Derivative</a></li>
        <li><a href="#duality">Duality</a></li>
        <li><a href="#mirror-decent">Mirror Decent</a>
          <ul>
            <li><a href="#analysis">Analysis</a></li>
          </ul>
        </li>
        <li><a href="#gradient-decent">Gradient Decent</a></li>
        <li><a href="#accelerated-gradient-descent">Accelerated Gradient Descent</a>
          <ul>
            <li><a href="#analysis-1">Analysis</a></li>
            <li><a href="#when-function-is-both-lipschitz-and-strongly-convex">When Function is both Lipschitz and Strongly Convex</a></li>
            <li><a href="#application-solve-linear-system">Application: Solve Linear System</a></li>
          </ul>
        </li>
        <li><a href="#newtons-method">Newton&rsquo;s Method</a>
          <ul>
            <li><a href="#intuition-1">Intuition 1</a></li>
            <li><a href="#intuition-2">Intuition 2</a></li>
            <li><a href="#ne-condition">NE condition</a></li>
            <li><a href="#intuition-3">Intuition 3</a></li>
            <li><a href="#nl-condition">NL condition</a></li>
          </ul>
        </li>
        <li><a href="#interior-node-method">Interior Node Method</a></li>
      </ul>
    </li>
  </ul>
</nav>
</div>
<script>
    window.addEventListener('DOMContentLoaded', () => {
        enableStickyToc();
    });
</script>
                
            </div>
            
        </div>

    </div>


</div>

<script>
    document.addEventListener('DOMContentLoaded', () => {
        hljs.initHighlightingOnLoad();
        changeSidebarHeight();
        switchDocToc();
    })
</script>








  </main>
  <footer class="pl-scrollbar">
    <div class="w-full max-w-screen-xl mx-auto"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">Powered by <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a>. Last updated in 2025 June. </p>
</div></div>
  </footer>
</body>

</html>